{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.storage._lc_store import create_kv_docstore\n",
    "from langchain.storage import InMemoryStore, LocalFileStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://dalle3-swo.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"e51119f8d8774069a6594d92ccf7a70d\"\n",
    "\n",
    "\n",
    "# LLM\n",
    "gpt_4o = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    temperature=0\n",
    ")\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    temperature=0\n",
    ")\n",
    "smart_llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    temperature=0\n",
    ")\n",
    "# Embedding\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "embeddings_3_large = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-large\",\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "# load from disk and recreate retriever\n",
    "vectorstore_chunk_zie_400 = Chroma(\n",
    "    persist_directory=\"./chroma_db/su_embedding_400_large_with_source\", embedding_function=embeddings_3_large\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "# store = InMemoryStore()\n",
    "fs = LocalFileStore(\n",
    "    \"./parent_document_store/su_embedding_large_with_source\")\n",
    "store = create_kv_docstore(fs)\n",
    "parent_document_retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore_chunk_zie_400,\n",
    "    docstore=store,\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs_with_sources(docs: List[Document]) -> str:\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_str = f\"\"\"\\\n",
    "        Source Name: {doc.metadata['file_name']} - Page {doc.metadata['page']}\n",
    "        Information: {doc.page_content}\n",
    "        \"\"\"\n",
    "        formatted.append(doc_str)\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "As an AI assistant specializing in student support, your task is to provide concise and comprehensive answers to specific questions based on the provided context. \n",
    "The context is a list of sources. Each source includes source name and information.\n",
    "You MUST follow instruction deliminated by ###.\n",
    "\n",
    "###\n",
    "Instructions:\n",
    "\n",
    "1. Begin by reading the context carefully.\n",
    "2. Answer the question based on the information in the context.\n",
    "3. If you don’t know the answer, say \"Sorry, the documents do not mention about this information. Please contact the Student Information Office via studentservice@buv.edu.vn for further support. Thank you\". Do not fabricate responses. And Do not make up references\n",
    "4. Keep your answer as succinct as possible, but ensure it includes all relevant information from the context. For examples: \n",
    "    - if students ask about a department or services, you should answer not only department name or serivec name, but also service link and department contact such as email, phone, ... if those information have in the context. \n",
    "    - if context does not have specific answer, but contain reference information such as reference link, reference contact point, support contact point and so on. Then you should show it up.\n",
    "    - if context contains advices for specific student's action, you should show it up.\n",
    "5. Always include the source name from the context for each fact you use in the response in the following format: \n",
    "```\n",
    "{{Answer here}} \n",
    "\n",
    "Sources:\n",
    "- Source name 1\n",
    "- Source name 2\n",
    "....\n",
    "- Source name n\n",
    "```\n",
    "### \n",
    "\n",
    "--- Start Context:\n",
    "{context}\n",
    "--- End Context\n",
    "\n",
    "Note that if the previous conversations contains usefull information, you can response based on provided context and those information too. \n",
    "Only answer in English.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "parent_document_with_history_aware_retriever = create_history_aware_retriever(\n",
    "    gpt_4o, parent_document_retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "custom_retriever_chain: Runnable = parent_document_with_history_aware_retriever | format_docs_with_sources\n",
    "\n",
    "rag_chain_with_parent_retriever_with_sources: Runnable = (\n",
    "    RunnablePassthrough.assign(context=custom_retriever_chain)\n",
    "    | qa_prompt\n",
    "    | gpt_35_turbo_16k\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "chain_with_message_history: Runnable = RunnableWithMessageHistory(\n",
    "    rag_chain_with_parent_retriever_with_sources,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def trim_messages(chain_input):\n",
    "    # just get two latest conversation\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) <= 2:\n",
    "        return False\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    for message in stored_messages[-2:]:\n",
    "        demo_ephemeral_chat_history.add_message(message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_trimming = (\n",
    "    RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_test(query, system_prompt=system_prompt,instruction_in_system=True, llm=gpt_35_turbo_16k, context_aware_retriever_llm = gpt_35_turbo_16k, memory=demo_ephemeral_chat_history):\n",
    "    # retriever with history aware\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    parent_document_with_history_aware_retriever = create_history_aware_retriever(\n",
    "        context_aware_retriever_llm, parent_document_retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    # main chain\n",
    "    if instruction_in_system:\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"human\", system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )      \n",
    "\n",
    "    custom_retriever_chain = parent_document_with_history_aware_retriever | format_docs_with_sources\n",
    "\n",
    "    rag_chain_with_parent_retriever_with_sources = (\n",
    "        RunnablePassthrough.assign(context=custom_retriever_chain)\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    chain_with_message_history = RunnableWithMessageHistory(\n",
    "        rag_chain_with_parent_retriever_with_sources,\n",
    "        lambda session_id: memory,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "    def trim_messages(chain_input):\n",
    "        stored_messages = demo_ephemeral_chat_history.messages\n",
    "        if len(stored_messages) <= 2:\n",
    "            return False\n",
    "\n",
    "        demo_ephemeral_chat_history.clear()\n",
    "\n",
    "        for message in stored_messages[-2:]:\n",
    "            demo_ephemeral_chat_history.add_message(message)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    chain_with_follow_up = (\n",
    "        RunnablePassthrough.assign(messages_trimmed=trim_messages)\n",
    "        | chain_with_message_history\n",
    "    )\n",
    "\n",
    "    response = chain_with_follow_up.invoke(\n",
    "    {\"input\": query},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ephemeral_chat_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"I want to register to study late at the university\",\n",
    "    \"Is my reason valid for approval?\",\n",
    "    \"What should I do to be absent for a driving test session?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 6c8e15a5-e950-49c2-94fd-e6f9a96d18b3 not found for run 3431bacb-169c-4139-8531-00e301b623cb. Treating as a root run.\n"
     ]
    }
   ],
   "source": [
    "a = chain_test(query=questions[0], \n",
    "               instruction_in_system=False, \n",
    "               llm=gpt_35_turbo_16k, \n",
    "               context_aware_retriever_llm=gpt_4o,\n",
    "               system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can register to study at the university outside of regular operating hours. The university allows students to access classrooms and facilities for self-study purposes during both operating and out-of-hours time. \\n\\nTo register for accessing campus facilities during operating hours:\\n- Check the availability of accessible rooms.\\n- Each student can use rooms for a maximum of 1 hour per day and 2 hours per week.\\n- Register directly at the Student Information Office on level 2.\\n\\nTo register for accessing campus facilities outside of operating hours:\\n- Check the availability of accessible rooms (room 3-5, 3-6, 3-7, 3-8, 3-11, 3-12) and functional rooms (approval required from the relevant Discipline Lead).\\n- Submit the Out-of-hours Access Agreement form at the Student Information Office before 4:00 PM, Monday to Friday.\\n- For functional rooms, seek confirmation from the relevant Discipline Lead before submitting the form.\\n\\nPlease note that the approval for accessing registered rooms depends on availability. If you have any further questions, you can contact the Student Information Office at studentservice@buv.edu.vn.\\n\\nSources:\\n- SU-JUL24-FAQ.pdf - Page 38-39'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, the documents do not mention about this information. Please contact the Student Information Office via studentservice@buv.edu.vn for further support. Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = parent_document_retriever.invoke(\"Can I retake this exam?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='How are retakes at BUV regulated? / Quy định học lại ở BUV như thế nào?\\n\\nAnswer / Câu trả lời:\\n\\nIf the External Examination Board decides that student needs to retake a module, students will need to study that module again with all achieved marked wiped clean. The student will also need to pay the restudy fee for the module(s) & ensure attendance. Their mark will not be capped for the first retake attempt, but any second attempt is capped at 40% for undergraduate level. On the other hand, a student’s Academic Misconduct history is retained on file and is taken into account on further academic conduct instances.\\n\\nNếu Hội đồng Khảo thí quyết định sinh viên cần phải học lại một hoặc nhiều môn, toàn bộ điểm số trước đó sinh viên đã đạt được trong môn học sẽ bị xóa khỏi hệ thống và sinh viên sẽ cần học lại môn học từ đầu khi có cơ hội kế tiếp, đảm bảo chuyên cần và thanh toán phí học lại. Khi học lại, điểm sẽ được tính như bình thường. Nếu sinh viên thi trượt và thi lại môn học lại, quy định về điểm thi lại tối đa là 40% được áp dụng. Bên cạnh đó, số lần vi phạm quy định học thuật của sinh viên sẽ được lưu tại hồ sơ và là cơ sở cho các quyết định liên quan đến vi phạm học thuật sau đó.', metadata={'file_name': 'SU-JUL24-FAQ.pdf', 'page': '51'}),\n",
       " Document(page_content='Can the student take the resit instead of restudying? / Sinh viên có thể thi lại thay vì học lại không?\\n\\nAnswer / Câu trả lời:\\n\\nResit/restudy is not an option. The Examinations Board has carefully considered each case and given the decision whether the student needs to restudy or resit to complete the module. After the final decision is made, it cannot be changed. Thus, the student should read and follow the Board decision presented in their academic results.\\n\\nThi lại/ học lại không phải là một lựa chọn. Hội Đồng Khảo Thí đã xem xét kỹ lưỡng từng trường hợp và đưa ra quyết định sinh viên cần học lại hay thi lại để hoàn thành môn học. Sẽ không thể thay đổi quyết định cuối cùng của Hội Đồng Khảo Thí. Do đó, sinh viên cần đọc và thực hiện theo quyết định của Hội Đồng Khảo Thí được nêu trong bảng điểm.', metadata={'file_name': 'SU-JUL24-FAQ.pdf', 'page': '65'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
